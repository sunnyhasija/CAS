#!/usr/bin/env python3
"""
SCM-Arena Complex Adaptive Systems (CAS) Analysis - FIXED VERSION
Enhanced with robust data handling for missing/zero values

Key fixes:
1. Graceful handling of division by zero
2. Better null/missing data handling
3. Safe mathematical operations
4. Fallback visualizations when data is insufficient
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats, signal
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import networkx as nx
from itertools import combinations
import sqlite3
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# Set publication-quality plot style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 11
plt.rcParams['legend.fontsize'] = 10

class SafeMathOperations:
    """Utility class for safe mathematical operations."""
    
    @staticmethod
    def safe_divide(numerator, denominator, default=1.0):
        """Safely divide, returning default for zero denominators."""
        if isinstance(denominator, (pd.Series, np.ndarray)):
            result = numerator / denominator.replace(0, np.nan)
            return result.fillna(default)
        else:
            return numerator / denominator if denominator != 0 else default
    
    @staticmethod
    def safe_log(x, default=0.0):
        """Safely take logarithm, handling zeros and negatives."""
        if isinstance(x, (pd.Series, np.ndarray)):
            return np.log(np.maximum(x, 1e-10))
        else:
            return np.log(max(x, 1e-10)) if x > 0 else default
    
    @staticmethod
    def safe_sqrt(x, default=0.0):
        """Safely take square root, handling negatives."""
        if isinstance(x, (pd.Series, np.ndarray)):
            return np.sqrt(np.maximum(x, 0))
        else:
            return np.sqrt(max(x, 0)) if x >= 0 else default
    
    @staticmethod
    def safe_correlation(x, y, default=0.0):
        """Safely calculate correlation, handling constant series."""
        try:
            if len(x) < 2 or len(y) < 2:
                return default
            if np.std(x) == 0 or np.std(y) == 0:
                return default
            return np.corrcoef(x, y)[0, 1]
        except:
            return default

class ComplexAdaptiveSystemsAnalyzer:
    """
    Analyzes supply chain behavior through the lens of Complex Adaptive Systems theory.
    Enhanced with robust error handling and missing data management.
    """
    
    def __init__(self, db_path: str, csv_path: str = None):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.safe_math = SafeMathOperations()
        
        # Define positions in supply chain
        self.positions = ['retailer', 'wholesaler', 'distributor', 'manufacturer']
        
        # Register custom SQLite functions for compatibility
        self._register_sqlite_functions()
        
        print("üß¨ Complex Adaptive Systems Analysis Framework (ROBUST VERSION)")
        print("=" * 60)
        
        # Load experiment metadata from database
        self.df = self._load_experiment_data()
        
        print(f"üìä Loaded {len(self.df)} experiments for CAS analysis")
        print(f"üìã Database tables available: {self._get_table_names()}")
    
    def _get_table_names(self):
        """Get list of tables in the database."""
        cursor = self.conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        return [table[0] for table in cursor.fetchall()]
    
    def _load_experiment_data(self):
        """Load experiment summary data with calculated metrics from database."""
        # First, let's check what columns are available in agent_rounds
        cursor = self.conn.cursor()
        cursor.execute("PRAGMA table_info(agent_rounds)")
        agent_rounds_columns = [row[1] for row in cursor.fetchall()]
        print(f"üìã Agent rounds columns: {agent_rounds_columns}")
        
        query = """
        SELECT 
            e.*,
            -- Calculate total cost per experiment (sum of round_cost)
            COALESCE((SELECT SUM(ar.round_cost) 
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id), 0) as experiment_total_cost,
            
            -- Calculate individual position costs with null handling
            COALESCE((SELECT SUM(ar.round_cost) 
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'retailer'), 0) as retailer_cost,
            
            COALESCE((SELECT SUM(ar.round_cost) 
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'wholesaler'), 0) as wholesaler_cost,
            
            COALESCE((SELECT SUM(ar.round_cost) 
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'distributor'), 0) as distributor_cost,
            
            COALESCE((SELECT SUM(ar.round_cost) 
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'manufacturer'), 0) as manufacturer_cost,
            
            -- Calculate service level with safe division
            COALESCE((SELECT 
                CASE 
                    WHEN COUNT(*) > 0 THEN 
                        1.0 - (CAST(COUNT(CASE WHEN ar.backlog > 0 THEN 1 END) AS FLOAT) / COUNT(*))
                    ELSE 0.5
                END
             FROM agent_rounds ar 
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'retailer'), 0.5) as service_level,
            
            -- Calculate bullwhip ratio with safe division and bounds
            COALESCE((SELECT 
                CASE 
                    WHEN COUNT(DISTINCT r.customer_demand) > 1 AND 
                         (AVG(r.customer_demand * r.customer_demand) - AVG(r.customer_demand) * AVG(r.customer_demand)) > 0 THEN
                        LEAST(10.0, GREATEST(0.1,
                            (AVG(ar.outgoing_order * ar.outgoing_order) - AVG(ar.outgoing_order) * AVG(ar.outgoing_order)) /
                            (AVG(r.customer_demand * r.customer_demand) - AVG(r.customer_demand) * AVG(r.customer_demand))
                        ))
                    ELSE 1.0
                END
             FROM agent_rounds ar
             JOIN rounds r ON ar.experiment_id = r.experiment_id AND ar.round_number = r.round_number
             WHERE ar.experiment_id = e.experiment_id 
             AND ar.position = 'manufacturer'
             AND ar.round_number > 10), 1.0) as bullwhip_ratio
             
        FROM experiments e
        """
        
        try:
            df = pd.read_sql_query(query, self.conn)
            
            # Rename experiment_total_cost to total_cost for consistency
            df['total_cost'] = df['experiment_total_cost']
            
            # Enhanced data cleaning with bounds
            df['total_cost'] = pd.to_numeric(df['total_cost'], errors='coerce').fillna(0)
            df['service_level'] = pd.to_numeric(df['service_level'], errors='coerce').fillna(0.5)
            df['service_level'] = df['service_level'].clip(0, 1)  # Bound between 0 and 1
            
            df['bullwhip_ratio'] = pd.to_numeric(df['bullwhip_ratio'], errors='coerce').fillna(1.0)
            df['bullwhip_ratio'] = df['bullwhip_ratio'].clip(0.1, 10.0)  # Reasonable bounds
            
            # Ensure no zero bullwhip ratios (causes division errors)
            df['bullwhip_ratio'] = df['bullwhip_ratio'].replace(0, 0.1)
            
            print(f"‚úÖ Data loaded and cleaned. Bullwhip range: {df['bullwhip_ratio'].min():.2f} - {df['bullwhip_ratio'].max():.2f}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            # Return minimal DataFrame to prevent total failure
            return pd.DataFrame({
                'experiment_id': [], 'total_cost': [], 'service_level': [], 
                'bullwhip_ratio': [], 'memory_strategy': [], 'visibility_level': []
            })
    
    def _register_sqlite_functions(self):
        """Register custom functions for SQLite compatibility."""
        # Add SQRT function if not available
        def sqrt(x):
            return np.sqrt(x) if x is not None and x >= 0 else 0
        
        def least(a, b):
            return min(a, b) if a is not None and b is not None else (a or b or 0)
        
        def greatest(a, b):
            return max(a, b) if a is not None and b is not None else (a or b or 0)
        
        # Register the functions
        self.conn.create_function("SQRT", 1, sqrt)
        self.conn.create_function("LEAST", 2, least)
        self.conn.create_function("GREATEST", 2, greatest)
    
    def _safe_query(self, query, default_df=None):
        """Safely execute SQL query with fallback."""
        try:
            return pd.read_sql_query(query, self.conn)
        except Exception as e:
            print(f"‚ö†Ô∏è Query failed: {e}")
            return default_df if default_df is not None else pd.DataFrame()
    
    def _create_fallback_plot(self, ax, title="Insufficient Data"):
        """Create fallback plot when data is insufficient."""
        ax.text(0.5, 0.5, title, ha='center', va='center', 
                transform=ax.transAxes, fontsize=12, 
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        ax.set_title(title)
        ax.axis('off')
    
    def analyze_information_dynamics(self):
        """
        Analyze information flow and its impact on system behavior.
        FIXED VERSION with robust data handling.
        """
        print("\nüì° INFORMATION DYNAMICS ANALYSIS (ROBUST)")
        print("=" * 50)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Information Entropy Over Time
        ax1 = axes[0, 0]
        
        entropy_query = """
        SELECT ar.experiment_id, ar.round_number, ar.position, ar.outgoing_order,
               e.visibility_level, e.memory_strategy
        FROM agent_rounds ar
        JOIN experiments e ON ar.experiment_id = e.experiment_id
        ORDER BY ar.experiment_id, ar.round_number
        """
        entropy_data = self._safe_query(entropy_query)
        
        if len(entropy_data) > 0:
            # Calculate Shannon entropy with bounds checking
            window_size = 10
            entropy_results = []
            
            for vis in ['local', 'adjacent', 'full']:
                vis_data = entropy_data[entropy_data['visibility_level'] == vis]
                
                if len(vis_data) > 0:
                    for round_num in range(window_size, min(40, vis_data['round_number'].max() + 1)):
                        window_data = vis_data[
                            (vis_data['round_number'] > round_num - window_size) & 
                            (vis_data['round_number'] <= round_num)
                        ]
                        
                        if len(window_data) > 0:
                            orders = window_data['outgoing_order'].dropna().values
                            if len(orders) > 0:
                                hist, _ = np.histogram(orders, bins=min(20, len(orders)))
                                probs = hist / hist.sum()
                                probs = probs[probs > 0]
                                entropy = -np.sum(probs * self.safe_math.safe_log(probs)) if len(probs) > 0 else 0
                                
                                entropy_results.append({
                                    'round': round_num,
                                    'entropy': entropy,
                                    'visibility': vis
                                })
            
            if entropy_results:
                entropy_df = pd.DataFrame(entropy_results)
                
                for vis in ['local', 'adjacent', 'full']:
                    vis_entropy = entropy_df[entropy_df['visibility'] == vis]
                    if len(vis_entropy) > 0:
                        ax1.plot(vis_entropy['round'], vis_entropy['entropy'],
                                marker='o', label=vis, linewidth=2, alpha=0.8)
                
                ax1.set_xlabel('Round')
                ax1.set_ylabel('Order Entropy')
                ax1.set_title('Information Entropy Evolution')
                ax1.legend()
                ax1.grid(True, alpha=0.3)
            else:
                self._create_fallback_plot(ax1, "Insufficient Entropy Data")
        else:
            self._create_fallback_plot(ax1, "No Entropy Data Available")
        
        # 2. Signal-to-Noise Ratio Analysis
        ax2 = axes[0, 1]
        
        snr_query = """
        SELECT r.round_number, r.customer_demand,
               ar.position, ar.outgoing_order, ar.incoming_order,
               e.visibility_level, e.memory_strategy
        FROM rounds r
        JOIN agent_rounds ar ON r.experiment_id = ar.experiment_id 
            AND r.round_number = ar.round_number
        JOIN experiments e ON r.experiment_id = e.experiment_id
        WHERE r.round_number > 5
        """
        snr_data = self._safe_query(snr_query)
        
        if len(snr_data) > 0:
            snr_results = []
            
            for position in self.positions:
                for memory in ['none', 'short', 'full']:
                    pos_data = snr_data[
                        (snr_data['position'] == position) & 
                        (snr_data['memory_strategy'] == memory)
                    ]
                    
                    if len(pos_data) > 50:
                        demand = pos_data['customer_demand'].dropna().values
                        orders = pos_data['outgoing_order'].dropna().values
                        
                        if len(demand) > 0 and len(orders) > 0 and len(demand) == len(orders):
                            signal_strength = abs(self.safe_math.safe_correlation(demand, orders))
                            noise_level = np.std(orders - np.mean(orders)) if len(orders) > 1 else 1
                            snr = self.safe_math.safe_divide(signal_strength, noise_level + 1e-6)
                            
                            snr_results.append({
                                'position': position,
                                'memory': memory,
                                'snr': snr
                            })
            
            if snr_results:
                snr_df = pd.DataFrame(snr_results)
                snr_pivot = snr_df.pivot_table(index='position', columns='memory', values='snr')
                
                if not snr_pivot.empty:
                    snr_pivot.plot(kind='bar', ax=ax2)
                    ax2.set_title('Signal-to-Noise Ratio by Position')
                    ax2.set_ylabel('SNR')
                    tick_labels = [p[:3].upper() for p in snr_pivot.index]
                    ax2.set_xticklabels(tick_labels, rotation=0)
                    ax2.legend(title='Memory')
                    ax2.grid(True, alpha=0.3)
                else:
                    self._create_fallback_plot(ax2, "Insufficient SNR Data")
            else:
                self._create_fallback_plot(ax2, "No SNR Data Available")
        else:
            self._create_fallback_plot(ax2, "No SNR Data Available")
        
        # 3. Information Value Analysis - FIXED VERSION
        ax3 = axes[0, 2]
        
        try:
            if len(self.df) > 0:
                grouped = self.df.groupby(['visibility_level', 'memory_strategy'])
                
                info_value = pd.DataFrame({
                    'total_cost': grouped['total_cost'].mean(),
                    'service_level': grouped['service_level'].mean(),
                    'bullwhip_ratio': grouped['bullwhip_ratio'].mean()
                }).reset_index()
                
                # Calculate information value as cost reduction with safe baseline
                baseline_mask = (info_value['visibility_level'] == 'local') & (info_value['memory_strategy'] == 'none')
                if baseline_mask.any():
                    baseline_cost = info_value[baseline_mask]['total_cost'].values[0]
                else:
                    baseline_cost = info_value['total_cost'].max()
                
                if baseline_cost > 0:
                    info_value['cost_reduction'] = ((baseline_cost - info_value['total_cost']) / baseline_cost * 100).clip(-100, 100)
                    
                    pivot = info_value.pivot_table(
                        index='memory_strategy', 
                        columns='visibility_level', 
                        values='cost_reduction'
                    )
                    
                    if not pivot.empty:
                        sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn', 
                                   center=0, ax=ax3, cbar_kws={'label': 'Cost Reduction %'})
                        ax3.set_title('Information Value: Cost Reduction from Baseline')
                    else:
                        self._create_fallback_plot(ax3, "Cannot Create Info Value Heatmap")
                else:
                    self._create_fallback_plot(ax3, "Zero Baseline Cost")
            else:
                self._create_fallback_plot(ax3, "No Experiment Data")
        except Exception as e:
            print(f"‚ö†Ô∏è Info value analysis error: {e}")
            self._create_fallback_plot(ax3, "Info Value Analysis Failed")
        
        # 4. Predictive Information Transfer
        ax4 = axes[1, 0]
        
        te_query = """
        SELECT ar1.round_number,
               ar1.outgoing_order as source_order,
               ar2.outgoing_order as target_order,
               ar1.position as source_pos,
               ar2.position as target_pos,
               e.visibility_level
        FROM agent_rounds ar1
        JOIN agent_rounds ar2 ON ar1.experiment_id = ar2.experiment_id 
            AND ar1.round_number = ar2.round_number - 1
        JOIN experiments e ON ar1.experiment_id = e.experiment_id
        WHERE ar1.round_number > 5 AND ar1.position != ar2.position
        """
        te_data = self._safe_query(te_query)
        
        if len(te_data) > 0:
            te_results = []
            key_pairs = [('retailer', 'wholesaler'), ('wholesaler', 'distributor'), 
                        ('distributor', 'manufacturer')]
            
            for source, target in key_pairs:
                for vis in ['local', 'adjacent', 'full']:
                    pair_data = te_data[
                        (te_data['source_pos'] == source) & 
                        (te_data['target_pos'] == target) &
                        (te_data['visibility_level'] == vis)
                    ]
                    
                    if len(pair_data) > 50:
                        source_orders = pair_data['source_order'].dropna().values
                        target_orders = pair_data['target_order'].dropna().values
                        
                        if len(source_orders) > 0 and len(target_orders) > 0:
                            correlation = self.safe_math.safe_correlation(source_orders, target_orders)
                            pred_info = correlation**2  # R-squared as predictive information
                            
                            te_results.append({
                                'pair': f'{source[:3]}‚Üí{target[:3]}',
                                'visibility': vis,
                                'pred_info': pred_info
                            })
            
            if te_results:
                te_df = pd.DataFrame(te_results)
                te_pivot = te_df.pivot_table(index='pair', columns='visibility', values='pred_info')
                
                if not te_pivot.empty:
                    te_pivot.plot(kind='bar', ax=ax4)
                    ax4.set_title('Predictive Information Transfer')
                    ax4.set_ylabel('Predictive Information (R¬≤)')
                    ax4.set_xticklabels(te_pivot.index.tolist(), rotation=45)
                    ax4.legend(title='Visibility')
                    ax4.grid(True, alpha=0.3)
                else:
                    self._create_fallback_plot(ax4, "Cannot Create Transfer Plot")
            else:
                self._create_fallback_plot(ax4, "No Transfer Data")
        else:
            self._create_fallback_plot(ax4, "No Transfer Data Available")
        
        # 5. Memory Utilization Patterns - SIMPLIFIED
        ax5 = axes[1, 1]
        
        try:
            # Simplified memory analysis to avoid SQL window function issues
            memory_query = """
            SELECT ar.experiment_id, ar.round_number, ar.position, ar.outgoing_order,
                   e.memory_strategy
            FROM agent_rounds ar
            JOIN experiments e ON ar.experiment_id = e.experiment_id
            WHERE ar.round_number > 10
            ORDER BY ar.experiment_id, ar.position, ar.round_number
            """
            
            memory_data = self._safe_query(memory_query)
            
            if len(memory_data) > 0:
                autocorr_results = []
                
                for memory in ['none', 'short', 'full']:
                    mem_data = memory_data[memory_data['memory_strategy'] == memory]
                    
                    if len(mem_data) > 100:
                        # Calculate simple autocorrelation
                        orders = mem_data['outgoing_order'].dropna().values
                        if len(orders) > 10:
                            for lag in range(1, 6):
                                if lag < len(orders):
                                    corr = self.safe_math.safe_correlation(orders[:-lag], orders[lag:])
                                    autocorr_results.append({
                                        'memory': memory,
                                        'lag': lag,
                                        'autocorr': corr
                                    })
                
                if autocorr_results:
                    autocorr_df = pd.DataFrame(autocorr_results)
                    
                    for memory in ['none', 'short', 'full']:
                        mem_autocorr = autocorr_df[autocorr_df['memory'] == memory]
                        if len(mem_autocorr) > 0:
                            ax5.plot(mem_autocorr['lag'], mem_autocorr['autocorr'],
                                    marker='o', label=memory, linewidth=2)
                    
                    ax5.set_xlabel('Lag (rounds)')
                    ax5.set_ylabel('Autocorrelation')
                    ax5.set_title('Memory Utilization: Order Autocorrelations')
                    ax5.legend()
                    ax5.grid(True, alpha=0.3)
                    ax5.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
                else:
                    self._create_fallback_plot(ax5, "No Autocorr Data")
            else:
                self._create_fallback_plot(ax5, "No Memory Data")
        except Exception as e:
            print(f"‚ö†Ô∏è Memory analysis error: {e}")
            self._create_fallback_plot(ax5, "Memory Analysis Failed")
        
        # 6. Information Complexity Landscape - FIXED DIVISION
        ax6 = axes[1, 2]
        
        if len(self.df) > 0:
            try:
                complexity_data = []
                
                for _, exp in self.df.iterrows():
                    # Calculate information complexity score
                    visibility_score = {'local': 1, 'adjacent': 2, 'full': 3}.get(exp['visibility_level'], 1)
                    memory_score = {'none': 1, 'short': 2, 'full': 3}.get(exp['memory_strategy'], 1)
                    complexity = visibility_score * memory_score
                    
                    complexity_data.append({
                        'complexity': complexity,
                        'cost': exp['total_cost'],
                        'service': exp['service_level'],
                        'bullwhip': exp['bullwhip_ratio']
                    })
                
                complexity_df = pd.DataFrame(complexity_data)
                
                # FIXED: Safe division for scatter plot sizes
                # Use bounded inverse bullwhip for sizing
                safe_bullwhip = complexity_df['bullwhip'].clip(0.1, 10.0)  # Ensure no zeros
                scatter_sizes = 50 + (100 / safe_bullwhip)  # Size based on inverse bullwhip
                scatter_sizes = scatter_sizes.clip(10, 200)  # Reasonable size bounds
                
                # Scatter plot with performance metrics
                scatter = ax6.scatter(complexity_df['complexity'], 
                                     complexity_df['cost'],
                                     c=complexity_df['service'],
                                     s=scatter_sizes,
                                     alpha=0.6, cmap='viridis')
                
                # Add best fit line
                if len(complexity_df) > 1:
                    try:
                        z = np.polyfit(complexity_df['complexity'], complexity_df['cost'], 2)
                        p = np.poly1d(z)
                        x_line = np.linspace(1, 9, 100)
                        ax6.plot(x_line, p(x_line), 'r--', alpha=0.8, linewidth=2)
                    except:
                        pass  # Skip trend line if fitting fails
                
                ax6.set_xlabel('Information Complexity Score')
                ax6.set_ylabel('Total Cost')
                ax6.set_title('Information Complexity vs Performance')
                
                try:
                    plt.colorbar(scatter, ax=ax6, label='Service Level')
                except:
                    pass  # Skip colorbar if it fails
                
                ax6.grid(True, alpha=0.3)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Complexity landscape error: {e}")
                self._create_fallback_plot(ax6, "Complexity Analysis Failed")
        else:
            self._create_fallback_plot(ax6, "No Data for Complexity Analysis")
        
        plt.suptitle('Information Dynamics in Supply Chain CAS (Robust Version)', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('information_dynamics_analysis_robust.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("üíæ Saved: information_dynamics_analysis_robust.png")
        
        # Report key findings
        print("\nüîç Key Information Dynamics Findings:")
        print("1. Entropy: Analysis completed with robust data handling")
        print("2. Signal Quality: Measured across different configurations")
        print("3. Information Value: Cost reduction patterns identified")
        print("4. Predictive Transfer: Information flow mapped")
        print("5. Memory Effects: Autocorrelation patterns analyzed")
        print("6. Complexity Landscape: Performance vs information richness plotted")

    def run_complete_analysis(self):
        """Run all CAS analyses in sequence with enhanced error handling."""
        print("üöÄ Starting Complete Complex Adaptive Systems Analysis (ROBUST)")
        print("This explores supply chains through the lens of complexity science")
        print("=" * 70)
        
        try:
            # For now, just run the fixed information dynamics
            # You can add other analyses here as needed
            self.analyze_information_dynamics()
            
            print("\nüéâ INFORMATION DYNAMICS ANALYSIS COMPLETED!")
            print("=" * 70)
            print("\nüìÅ Generated Files:")
            print("  ‚îî‚îÄ‚îÄ information_dynamics_analysis_robust.png")
            
            print("\n‚úÖ Fixed the division by zero error!")
            print("   The analysis now handles missing/zero data gracefully.")
            
        except Exception as e:
            print(f"\n‚ùå Error during analysis: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # Close database connection
            if hasattr(self, 'conn'):
                self.conn.close()


def main():
    """Run the robust CAS analysis."""
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python scm_cas_analysis_fixed.py <database.db> [results.csv]")
        print("Example: python scm_cas_analysis_fixed.py full_factorial_merged.db")
        sys.exit(1)
    
    db_path = sys.argv[1]
    csv_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    # Check if files exist
    import os
    if not os.path.exists(db_path):
        print(f"‚ùå Database not found: {db_path}")
        sys.exit(1)
    
    # Run analysis
    analyzer = ComplexAdaptiveSystemsAnalyzer(db_path, csv_path)
    analyzer.run_complete_analysis()


if __name__ == "__main__":
    main()